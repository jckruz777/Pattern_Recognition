{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> HADOOP & SPARK en Computación de Alto Desempeño para Aprendizaje Automático y Reconocimiento de Patrones </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/relationship.png\" width=\"400\" height=\"400\" description=\"Relaciones entre Áreas de Estudio\" />\n",
    "    <br>\n",
    "    <c>\n",
    "    <em>Fig. 1: Relación entre Áreas de Estudio</em> [1]\n",
    "    </c>\n",
    "</p>\n",
    "\n",
    "### Machine Learning + Big Data:\n",
    "\n",
    "* Problemas de reconocimiento de patrones.\n",
    "\n",
    "\n",
    "* Conocer los datos con los que se trabaja:\n",
    "  * Análisis estadístico y gráfico.\n",
    "  * Limpieza de datos.\n",
    "  * Aumento del set:\n",
    "    * Reducir dimensionalidad y redundancia (PCA).\n",
    "    * Simplificar modelos.\n",
    "    * Re-escalar variables (normalización).\n",
    "  * Categorizar el problema:\n",
    "    * Por entrada: supervisado/no supervisado.\n",
    "    * Por salida: clasificación, regresión, _clustering_.\n",
    "  * Analizar restricciones:\n",
    "    * Capacidad de almacenamiento (local o en la nube).\n",
    "    * Velocidad del funcionamiento (tiempo real).\n",
    "    * Capacidad de cambio de modelo.\n",
    "  * Encontrar el algoritmo apropiado:\n",
    "    * Requiere de experiencia.\n",
    "    \n",
    "\n",
    "* Un set de datos más grande mejora la precisión de los modelos.\n",
    "  * Aumentan los requerimientos en capacidad de procesamiento. \n",
    "  \n",
    "### Objetivo:\n",
    "\n",
    "Dar a conocer un conjunto de herramientas para el trabajo con grandes volúmenes de datos de manera eficiente en Aprendizaje Automático y Reconocimiento de Patrones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/hadoop.png\" width=\"400\" height=\"400\" />\n",
    "    <br>\n",
    "</p>\n",
    "\n",
    "Apache Hadoop es un _framework_ de código abierto que admite aplicaciones distribuidas de uso intensivo de datos bajo la licencia Apache. \n",
    "\n",
    "* Sistema de archivos distribuido en cada nodo del cluster.\n",
    "   * __HDFS__ (Hadoop Distributed File System): entorno de programación MapReduce.\n",
    "* Aplicaciones trabajan en miles de computadoras independientes.\n",
    "* Soporta Petabytes de datos ($10^{15}$).\n",
    "\n",
    "### 1.1 MapReduce\n",
    "\n",
    "_Framework_ que proporciona un sistema de procesamiento de datos paralelo y distribuido.\n",
    "* No todos los problemas pueden resolverse eficientemente.\n",
    "* Orientado a resolver problemas con conjuntos de datos de gran tamaño.\n",
    "* Arquitectura maestro / esclavo.\n",
    "\n",
    "\n",
    "* __Map:__ Función que aplica un mapeo a cada elemento de la entrada de datos. \n",
    "  * Recibe como parámetro un par (clave, valor) y devuelve una lista de pares.\n",
    "  * Se agrupan todos los pares con la misma clave de todas las listas.\n",
    "  * Se crea un grupo por cada clave generada. \n",
    "  * No es necesario que el tipo de datos para la entrada coincida con la salida.\n",
    "  * No es necesario que las claves de salida sean únicas.\n",
    "  * Operación paralelizable.\n",
    "\n",
    "\n",
    "* __Reduce:__ Función que se aplica en paralelo para cada grupo creado por la función __Map()__.\n",
    "  * La función __Reduce()__ se llama una vez para cada clave única de la salida de la función __Map()__. \n",
    "  * Dicha clave se pasa junto a una lista de todos los valores asociados con la clave para que fusionar en un conjunto más pequeño de valores.\n",
    "\n",
    "#### Ejemplo:\n",
    "\n",
    "* Datos duplicados:\n",
    "\n",
    "| # |  Nombre  |\n",
    "|:-:|:--:|\n",
    "| 1 | X |\n",
    "| 2 | B |\n",
    "| 3 | B |\n",
    "| 4 | C |\n",
    "| 5 | B |\n",
    "| 6 | A |\n",
    "| 7 | X |\n",
    "| 8 | A |\n",
    "| 9 | C |\n",
    "\n",
    "\n",
    "* __Map:__ \n",
    "  * Se toman subconjuntos de datos aislando las entradas en él. \n",
    "  * Para cada entrada se crea una clave y un valor, conformando una tupla (k_i, v_i). \n",
    "  * Para cada clave k_i se asigna v_i como 1, porque se quiere contar la cantidad de repeticiones del nombre.\n",
    "\n",
    "------------------------------------------------\n",
    "```\n",
    "# Subset 1\n",
    "X  → (Key: 'X’, Value: ‘1’)\n",
    "B  → (Key: ‘B’, Value: ‘1’)\n",
    "B  → (Key: ‘B’, Value: ‘1’)\n",
    "\n",
    "# Subset 2\n",
    "C  → (Key: 'C’, Value: ‘1’)\n",
    "B  → (Key: ‘B’, Value: ‘1’)\n",
    "A  → (Key: ‘A’, Value: ‘1’)\n",
    "\n",
    "# Subset 3\n",
    "X  → (Key: 'X’, Value: ‘1’)\n",
    "A  → (Key: ‘A’, Value: ‘1’)\n",
    "C  → (Key: ‘C’, Value: ‘1’)\n",
    "\n",
    "```\n",
    "------------------------------------------------\n",
    "\n",
    "* __Reduce:__\n",
    "   * Se convierten los datos de las tuplas anteriores. \n",
    "   * Se reducen las tuplas para que contengan claves únicas y la suma de sus valores.\n",
    "\n",
    "------------------------------------------------\n",
    "```\n",
    "# Tuplas\n",
    "\n",
    "[(X, 1), (B, 1), (B, 1), (C, 1), (B, 1), (A, 1), (X, 1), (A, 1), (C, 1)]\n",
    "\n",
    "# Tuplas Reducidas\n",
    "\n",
    "[(A, 2), (B, 3), (C, 2), (X, 2)]\n",
    "```\n",
    "------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/MapReduce.jpg\" width=\"500\" height=\"500\" />\n",
    "    <br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apache Spark\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/spark.svg\" width=\"300\" height=\"300\" />\n",
    "    <br>\n",
    "</p>\n",
    "\n",
    "Motor de análisis unificado para el procesamiento de Big Data.\n",
    " * Módulos incorporados para streaming, SQL, aprendizaje automático y procesamiento de gráficos.\n",
    " * Más de 80 operadores de alto nivel para aplicaciones paralelas. \n",
    " * Uso interactivo desde los intérpretes de Scala, Python, R y SQL.\n",
    " * Conjunto de bibliotecas que incluye __MLlib__ para aprendizaje automático.\n",
    " * Destaca en cargas de trabajo de transmisión, consultas interactivas y aprendizaje de máquina.\n",
    " \n",
    " ### 2.1 RDD\n",
    " \n",
    " * __Aplicación Spark:__ programa controlador que ejecuta operaciones paralelas en un clúster.\n",
    " * __Abstracción RDD:__ set de datos distribuido tolerante a fallos. Pueden operarse en paralelo.\n",
    " * __Hadoop:__ Los RDD se crean mediante un archivo inicial en el sistema de archivos Hadoop.\n",
    " \n",
    " ### 2.2 PySpark\n",
    "\n",
    "API que expone el modelo de programación de Spark en Python.\n",
    "* Construido sobre la API de Java en Spark. \n",
    "* Datos se procesan en Python, se almacenan en caché y se distribuyen en la JVM. \n",
    "* Py4J permite que programas de Python accedan a objetos Java en JVM.\n",
    "\n",
    " ### 2.3 Spark ML\n",
    " \n",
    "Herramienta que estandariza las API para algoritmos de aprendizaje automático, facilitando la combinación de múltiples algoritmos en una sola canalización o flujo de trabajo. Trabaja con los siguientes conceptos fundamentales [5]:\n",
    "\n",
    "* __Dataset ML:__ Spark ML utiliza el SchemaRDD de Spark SQL como un conjunto de datos que puede contener una variedad de tipos de datos. \n",
    "  * Por ejemplo: un conjunto de datos con columnas que almacenan texto, vectores de atributos y predicciones.\n",
    "\n",
    "\n",
    "* __Transformador:__ Algoritmo que transforma un SchemaRDD en otro SchemaRDD. \n",
    "  * Por ejemplo: un modelo ML transforma un RDD con atributos en un RDD con predicciones.\n",
    "\n",
    "\n",
    "* __Estimador:__ Algoritmo ajustable en un SchemaRDD para producir un transformador.\n",
    "  * Por ejemplo: un algoritmo de aprendizaje es un estimador que se entrena en un conjunto de datos y produce un modelo.\n",
    "\n",
    "\n",
    "* __Pipeline:__ Encadena varios transformadores y estimadores para especificar un flujo de trabajo de ML.\n",
    "\n",
    "\n",
    "* __Parámetro:__ todos los transformadores y estimadores comparten una API común para especificar parámetros.\n",
    "\n",
    "\n",
    "#### Instalación:\n",
    "\n",
    "* Paquetes principales:\n",
    "--------------------------------------------------------------\n",
    "```\n",
    "conda install jupyter notebook\n",
    "conda install -c cyclus java-jdk\n",
    "conda install -c conda-forge pyspark\n",
    "```\n",
    "--------------------------------------------------------------\n",
    "\n",
    "* Descargar Apache Spark de [este enlace](http://spark.apache.org/downloads.html) e instalarlo (versión 2.4.0 en este caso):\n",
    "--------------------------------------------------------------\n",
    "```\n",
    "tar xvf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "mv spark-2.4.0-bin-hadoop2.7 /usr/local/spark\n",
    "```\n",
    "--------------------------------------------------------------\n",
    "\n",
    "* Abrir el archivo _.bashrc_:\n",
    "--------------------------------------------------------------\n",
    "```\n",
    "vi ~/.bashrc\n",
    "```\n",
    "--------------------------------------------------------------\n",
    "* Agregar el siguiente contenido:\n",
    "--------------------------------------------------------------\n",
    "```\n",
    "export SPARK_HOME=\"/usr/local/spark/\"\n",
    "```\n",
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop vs Spark\n",
    "\n",
    "Se presentan algunas diferencias en la siguiente tabla [2]:\n",
    "\n",
    "| # | Propiedad           |  Hadoop MapReduce  | Apache Spark |\n",
    "| -- | ---------------------- | -------------------------------- | ------------------------------- |\n",
    "| 1 | Velocidad           | Lecturas y escrituras a disco | 100x en memoria y 10x disco más rápido|\n",
    "| 2 | Dificultad          | Operaciones requieren mayor cantidad de código  | Gran cantidad de peradores de alto nivel |\n",
    "| 3 | Facilidad de Manejo | MapReduce solo proporciona motor por lotes: dependiencia de diferentes motores | Todo en el mismo clúster: No es necesario gestionar componentes diferentes para cada necesidad |\n",
    "| 4 | Tiempo Real         | Orientado a procesamiento por lotes | Soportado |\n",
    "| 5 | Tolerancia a Fallas | Soportado | Soportado |\n",
    "| 6 | Seguridad           | Más seguro: compatible con las listas de control de acceso (ACL), modelo de permisos de archivos tradicional | Solamente admite autenticación de contraseña secreta compartida |\n",
    "\n",
    "__Ojo al Dato:__ MapReduce y Spark son compatibles entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SparkML Demo\n",
    "\n",
    "### Objetivo:\n",
    "\n",
    "Presentar la implementación de un clasificador de autos para explorar la transformación de datos y los operadores de pipeline de Spark para Aprendizaje Automático (_SparkML_) en Python.  \n",
    "\n",
    "* Se presenta los resultados de crear estimadores con 'Decision Tree' y 'Random Forest'.\n",
    "* Este demo reproduce la experimentación realizada en [4].\n",
    "* El clasificador está dado por un pipeline de la siguiente manera:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/sparkml_pipe.png\" width=\"600\" height=\"600\" />\n",
    "        <br>\n",
    "    <c>\n",
    "    <em>Fig. 2: Pipeline clásico en SparkML</em> [3]\n",
    "    </c>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+----+-----+----+-----+\n",
      "|  _c0|  _c1|_c2| _c3|  _c4| _c5|  _c6|\n",
      "+-----+-----+---+----+-----+----+-----+\n",
      "|vhigh|vhigh|  2|   2|small| low|unacc|\n",
      "|vhigh|vhigh|  2|   2|small| med|unacc|\n",
      "|vhigh|vhigh|  2|   2|small|high|unacc|\n",
      "|vhigh|vhigh|  2|   2|  med| low|unacc|\n",
      "|vhigh|vhigh|  2|   2|  med| med|unacc|\n",
      "|vhigh|vhigh|  2|   2|  med|high|unacc|\n",
      "|vhigh|vhigh|  2|   2|  big| low|unacc|\n",
      "|vhigh|vhigh|  2|   2|  big| med|unacc|\n",
      "|vhigh|vhigh|  2|   2|  big|high|unacc|\n",
      "|vhigh|vhigh|  2|   4|small| low|unacc|\n",
      "|vhigh|vhigh|  2|   4|small| med|unacc|\n",
      "|vhigh|vhigh|  2|   4|small|high|unacc|\n",
      "|vhigh|vhigh|  2|   4|  med| low|unacc|\n",
      "|vhigh|vhigh|  2|   4|  med| med|unacc|\n",
      "|vhigh|vhigh|  2|   4|  med|high|unacc|\n",
      "|vhigh|vhigh|  2|   4|  big| low|unacc|\n",
      "|vhigh|vhigh|  2|   4|  big| med|unacc|\n",
      "|vhigh|vhigh|  2|   4|  big|high|unacc|\n",
      "|vhigh|vhigh|  2|more|small| low|unacc|\n",
      "|vhigh|vhigh|  2|more|small| med|unacc|\n",
      "+-----+-----+---+----+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Preparando el contexto\n",
    "conf = SparkConf().setAppName(\"pyspark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Cargando el set de datos\n",
    "carDF = sqlContext.read.format(\"csv\").load(\"car.txt\")\n",
    "carDF.show()\n",
    "carDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-----+\n",
      "|buying|maint|doors|persons|lug_boot|safety|class|\n",
      "+------+-----+-----+-------+--------+------+-----+\n",
      "| vhigh|vhigh|    2|      2|   small|   low|unacc|\n",
      "| vhigh|vhigh|    2|      2|   small|   med|unacc|\n",
      "| vhigh|vhigh|    2|      2|   small|  high|unacc|\n",
      "| vhigh|vhigh|    2|      2|     med|   low|unacc|\n",
      "| vhigh|vhigh|    2|      2|     med|   med|unacc|\n",
      "| vhigh|vhigh|    2|      2|     med|  high|unacc|\n",
      "| vhigh|vhigh|    2|      2|     big|   low|unacc|\n",
      "| vhigh|vhigh|    2|      2|     big|   med|unacc|\n",
      "| vhigh|vhigh|    2|      2|     big|  high|unacc|\n",
      "| vhigh|vhigh|    2|      4|   small|   low|unacc|\n",
      "| vhigh|vhigh|    2|      4|   small|   med|unacc|\n",
      "| vhigh|vhigh|    2|      4|   small|  high|unacc|\n",
      "| vhigh|vhigh|    2|      4|     med|   low|unacc|\n",
      "| vhigh|vhigh|    2|      4|     med|   med|unacc|\n",
      "| vhigh|vhigh|    2|      4|     med|  high|unacc|\n",
      "| vhigh|vhigh|    2|      4|     big|   low|unacc|\n",
      "| vhigh|vhigh|    2|      4|     big|   med|unacc|\n",
      "| vhigh|vhigh|    2|      4|     big|  high|unacc|\n",
      "| vhigh|vhigh|    2|   more|   small|   low|unacc|\n",
      "| vhigh|vhigh|    2|   more|   small|   med|unacc|\n",
      "+------+-----+-----+-------+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombrando las columnas\n",
    "carDF = carDF.selectExpr(\"_c0 as buying\", \"_c1 as maint\", \"_c2 as doors\", \"_c3 as persons\", \"_c4 as lug_boot\", \"_c5 as safety\", \"_c6 as class\")\n",
    "carDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-----+------------+\n",
      "|buying|maint|doors|persons|lug_boot|safety|class|buying_index|\n",
      "+------+-----+-----+-------+--------+------+-----+------------+\n",
      "| vhigh|vhigh|    2|      2|   small|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|   small|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|   small|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     med|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     med|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     med|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     big|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     big|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      2|     big|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|   small|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|   small|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|   small|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     med|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     med|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     med|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     big|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     big|   med|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|      4|     big|  high|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|   more|   small|   low|unacc|         3.0|\n",
      "| vhigh|vhigh|    2|   more|   small|   med|unacc|         3.0|\n",
      "+------+-----+-----+-------+--------+------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conversión de datos categóricos en índices: atributo 'class'\n",
    "# StringIndexer: estimador (creado con fit), algoritmo ajustable en un marco para producir un transformador.\n",
    "# Transformador: convierte un marco de datos en otro agregando columnas\n",
    "# Etiquetas de índice, se agregan metadatos a la columna etiquetada.\n",
    "# Se usa fit en el conjunto de datos completo para incluir todas las etiquetas en el índice.\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"class_index\").fit(carDF)\n",
    "\n",
    "# Se definen las columnas de atributos para incluirlos en el vector de atributos\n",
    "#featureCols = array(\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\")\n",
    "\n",
    "# Se construye un ensamblador para combinar todos los atributos en un solo vector\n",
    "#assembler = VectorAssembler(inputCols=[\"featureCols\"], outputCol=\"features_index\")\n",
    "\n",
    "# Se genera un marco de datos con todas las columnas de atributos en un vector columna\n",
    "#features = assembler.transform(carDF)\n",
    "\n",
    "# Mapeo a índices\n",
    "buyingIndexer = StringIndexer(inputCol=\"buying\", outputCol=\"buying_index\").fit(carDF)\n",
    "\n",
    "# Se muestra la transformación de 'buying'\n",
    "carDF2 = buyingIndexer.transform(carDF)\n",
    "carDF2.show()\n",
    "\n",
    "# Se realizan las demás transformaciones\n",
    "maintIndexer = StringIndexer(inputCol=\"maint\", outputCol=\"maint_index\").fit(carDF)\n",
    "doorsIndexer = StringIndexer(inputCol=\"doors\", outputCol=\"doors_index\").fit(carDF)\n",
    "personsIndexer = StringIndexer(inputCol=\"persons\", outputCol=\"persons_index\").fit(carDF)\n",
    "lugbootIndexer = StringIndexer(inputCol=\"lug_boot\", outputCol=\"lug_boot_index\").fit(carDF)\n",
    "saftyIndexer = StringIndexer(inputCol=\"safety\", outputCol=\"safety_index\").fit(carDF)\n",
    "\n",
    "# Se agrupan en un único vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"buying_index\", \"maint_index\", \"doors_index\",\"persons_index\", \"lug_boot_index\", \"safety_index\"],\n",
    "    outputCol=\"features_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Árbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+\n",
      "|prediction|class_index|      features_index|\n",
      "+----------+-----------+--------------------+\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,1.0,...|\n",
      "|       0.0|        1.0| (6,[2,4],[2.0,2.0])|\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,0.0,...|\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,0.0,...|\n",
      "|       0.0|        0.0| (6,[2,5],[2.0,1.0])|\n",
      "+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.128492 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_5205887044f7) of depth 5 with 15 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Separación de los datos en conjuntos de prueba y entrenamiento (30% prueba).\n",
    "(trainingData, testData) = carDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Se crea una instancia (estimador) del modelo DecisionTree.\n",
    "dt = DecisionTreeClassifier(labelCol=\"class_index\", featuresCol=\"features_index\")\n",
    "\n",
    "# Se encadenan indexadores y un árbol al pipeline  \n",
    "pipeline_dt = Pipeline(stages=[labelIndexer, buyingIndexer, maintIndexer, doorsIndexer, personsIndexer, lugbootIndexer, saftyIndexer, assembler, dt])\n",
    " \n",
    "# Se realiza la transformación de datos y se inicia el aprendizaje con un modelo de árbol de decisión. \n",
    "# Se utilizan los parámetros almacenados.\n",
    "model_dt = pipeline_dt.fit(trainingData)\n",
    "\n",
    "# Se inician las predicciones.\n",
    "predictions = model_dt.transform(testData)\n",
    "\n",
    "# Impresión de los primeros resultados.\n",
    "predictions.select(\"prediction\", \"class_index\", \"features_index\").show(5)\n",
    "\n",
    "# Se obtiene el error después de evaluar.\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model_dt.stages[8]\n",
    "\n",
    "# Resumen\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-----+-----------+------------+-----------+-----------+-------------+--------------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|buying|maint|doors|persons|lug_boot|safety|class|class_index|buying_index|maint_index|doors_index|persons_index|lug_boot_index|safety_index|      features_index|       rawPrediction|         probability|prediction|\n",
      "+------+-----+-----+-------+--------+------+-----+-----------+------------+-----------+-----------+-------------+--------------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|  high| high|    2|      2|     big|   low|unacc|        0.0|         0.0|        0.0|        2.0|          1.0|           2.0|         2.0|[0.0,0.0,2.0,1.0,...| [403.0,0.0,0.0,0.0]|   [1.0,0.0,0.0,0.0]|       0.0|\n",
      "|  high| high|    2|      4|     big|  high|  acc|        1.0|         0.0|        0.0|        2.0|          0.0|           2.0|         0.0| (6,[2,4],[2.0,2.0])|[104.0,20.0,0.0,0.0]|[0.83870967741935...|       0.0|\n",
      "|  high| high|    2|      4|     med|   low|unacc|        0.0|         0.0|        0.0|        2.0|          0.0|           1.0|         2.0|[0.0,0.0,2.0,0.0,...| [258.0,0.0,0.0,0.0]|   [1.0,0.0,0.0,0.0]|       0.0|\n",
      "|  high| high|    2|      4|     med|   med|unacc|        0.0|         0.0|        0.0|        2.0|          0.0|           1.0|         1.0|[0.0,0.0,2.0,0.0,...|[104.0,20.0,0.0,0.0]|[0.83870967741935...|       0.0|\n",
      "|  high| high|    2|      4|   small|   med|unacc|        0.0|         0.0|        0.0|        2.0|          0.0|           0.0|         1.0| (6,[2,5],[2.0,1.0])|[104.0,20.0,0.0,0.0]|[0.83870967741935...|       0.0|\n",
      "+------+-----+-----+-------+--------+------+-----+-----------+------------+-----------+-----------+-------------+--------------+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualización de las nuevas columnas generadas por el transformador en el DataFrame original\n",
    "predictions.show (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+\n",
      "|prediction|class_index|      features_index|\n",
      "+----------+-----------+--------------------+\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,1.0,...|\n",
      "|       0.0|        1.0| (6,[2,4],[2.0,2.0])|\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,0.0,...|\n",
      "|       0.0|        0.0|[0.0,0.0,2.0,0.0,...|\n",
      "|       0.0|        0.0| (6,[2,5],[2.0,1.0])|\n",
      "+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.111732\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_81f6ca50c39a) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "# Incluyendo soporte para Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Creando una instancia (estimador) del modelo Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"class_index\", featuresCol=\"features_index\", numTrees=10)\n",
    "\n",
    "# Se encadenan indexadores y un bosque al pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, buyingIndexer, maintIndexer, doorsIndexer, personsIndexer, lugbootIndexer, saftyIndexer, assembler, rf])\n",
    "\n",
    "# Se realiza la transformación de datos y se inicia el aprendizaje con un modelo de Random Forest\n",
    "model = pipeline_rf.fit(trainingData)\n",
    "\n",
    "# Se inician las predicciones.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Impresión de los primeros resultados.\n",
    "predictions.select(\"prediction\", \"class_index\", \"features_index\").show(5)\n",
    "\n",
    "# Obtención del error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"class_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[8]\n",
    "\n",
    "# Resumen\n",
    "print(rfModel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Más Herramientas\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"IMG/similar.png\" width=\"550\" height=\"550\" />\n",
    "    <br>\n",
    "</p>\n",
    "\n",
    "* __Apache Flink:__ motor que proporciona distribución de datos, comunicación y tolerancia a fallas para computación distribuida sobre flujos de datos.\n",
    "\n",
    "\n",
    "* __Amazon Kinesis:__ plataforma para la transmisión de datos en AWS. Ofrece servicios potentes para facilitar la carga y el análisis de la transmisión de datos, y también brinda la posibilidad de crear aplicaciones personalizadas de transmisión de datos para necesidades especiales.\n",
    "\n",
    "\n",
    "* __Disco MapReduce:__ implementación de MapReduce para computación distribuida. Admite cálculos paralelos sobre grandes conjuntos de datos, almacenados en un grupo de computadoras no confiable, evitando preocuparse por protocolos de comunicación, equilibrio de carga, bloqueo, programación de tareas y la tolerancia a fallas.\n",
    "\n",
    "\n",
    "* __Apache Storm:__ sistema distribuido de computación en tiempo real de código abierto y gratuito. Facilita el procesamiento confiable de flujos de datos ilimitados, haciendo en tiempo real lo que Hadoop hace para el procesamiento por lotes.\n",
    "\n",
    "\n",
    "* __Heron:__ motor de procesamiento de flujo en tiempo real, distribuido y tolerante a fallos de Twitter.\n",
    "\n",
    "\n",
    "* __Gearpump:__ Apache Gearpump es un motor de transmisión de datos grandes en tiempo real. El motor está basado en eventos/mensajes. \n",
    "\n",
    "\n",
    "* __Upsolver:__ una plataforma de preparación de datos en memoria. Elimina la complejidad de los proyectos de Big Data y en tiempo real, acortando su tiempo de implementación de semanas/meses a horas. Está empaquetado como una solución de nube pública o privada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "* La calidad de los modelos mejora con Big Data.\n",
    "* Herramientas de alto nivel en la actualidad proveen capacidades de procesamiento favorables.\n",
    "* Conocer los datos con los que se trabaja y la naturaleza del problema es necesario para seleccionar las herramientas apropiadas.\n",
    "* Hadoop representa la base del almacenamiento distribuido de datos que puede utilizar Spark.\n",
    "* La combinación Hadoop/Spark requiere de un entorno HPC para demostrar su potencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "__[1]__ Trivedi, A. (2018). _How to become a Data Scientist?_. Obtenido del [enlace](https://medium.com/cutshort/how-to-become-a-data-scientist-a-detailed-step-by-step-guide-635b079937e2).\n",
    "\n",
    "__[2]__ Kahn, A. (2017). _What is the difference between Hadoop and Spark?_. Obtenido del [enlace](https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark)\n",
    "\n",
    "__[3]__ Yurong, F. (2017). _Introduction of a big data machine learning tool — SparkML_. Obtenido del [enlace](https://yurongfan.wordpress.com/2017/01/10/introduction-of-a-big-data-machine-learning-tool-sparkml/)\n",
    "\n",
    "__[4]__ Yurong, F. (2017). _SparkML Demo: Car classification using SparkML PySpark(Python API)_. Obtenido del [enlace](https://yurongfan.wordpress.com/2017/01/10/sparkml-demo-car-classification-using-sparkml-pysparkpython-api/)\n",
    "\n",
    "__[5]__ Apache Spark. (2018). _Spark ML Programming Guide_. Obtenido del [enlace](https://spark.apache.org/docs/1.2.2/ml-guide.html#main-concepts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
